{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TTS\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'His mother, Maria, was a homemaker. The family grew very much and there came a time when life became difficult for him. As a young girl, Noly got rid of a serious illness, so he started school late.'}]\n",
      "His mother, Maria, was a homemaker. The family grew very much and there came a time when life became difficult for him. As a young girl, Noly got rid of a serious illness, so he started school late.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate German to English. \n",
    "# If prompt is in english then no translate is needed\n",
    "# Output the result\n",
    "from langdetect import detect\n",
    "def translate_prompt(prompt):\n",
    "  is_prompt_english = detect(prompt) == 'en'\n",
    "  if not is_prompt_english:\n",
    "    # translate German to English\n",
    "    prompt_src_lang = 'als_Latn'\n",
    "    prompt_targt_lang = 'eng_Latn'\n",
    "    translator = pipeline('translation', model=model,\n",
    "                          tokenizer=tokenizer, src_lang=prompt_src_lang, tgt_lang=prompt_targt_lang)\n",
    "    output = translator(prompt, max_length=1024)\n",
    "    print(output)\n",
    "    prompt = output[0]['translation_text']\n",
    "    print(prompt)\n",
    "\n",
    "# Invoke the tranlsate-function and get results\n",
    "first_prompt = \"t. E ëma, Maria, ishte shtëpiake. Familja u shtua shumë dhe erdhi një kohë që jetesa u bë e vështirë për të. Sa qe i mitur Noli hoqi sëmundje të rënda, prandaj shkollën e nisi me vonesë. Filloren dhe të mesmen i bëri greqisht. Por me shumë ndikoi tek ai krenaria e fshatit shqiptar për të kaluarën historike dhe dashuria për shkrimin shqip.Libri i pare qe ka lexuar ne shqip ka qen Dhjata e Re me perkthim te Kristoforidhit.\"\n",
    "second_prompt = \"Beispiel 2: Ich bin ein Berliner.\"\n",
    "\n",
    "translate_prompt(first_prompt)\n",
    "#translate_prompt(second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[256162, 13374, 1398, 4260, 4039, 248130, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"How was your day?\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4050 Laptop GPU'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "im_cp = TTS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im_cp.subtitles_video_with_display('sample3.mp4',is_url=False,url='https://youtu.be/o-GdKXiM0gU',display=True,languag='en',save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_cp.translate = True\n",
    "#im_cp.translator.set_source_language ('eng_Latn')\n",
    "im_cp.translator.set_target_language ('ita_Latn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "im_cp.subtitles_video('sample4.mp4',chunk_length=-1,save_txt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'als_Latn'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import write_subs_to_video\n",
    "write_subs_to_video(video_fn=\"sample_tr_dize.mp4\",subs_fn=\"sample_tr_dize.srt\",output_path='sample_en_translated.mp4',font_sz=20,max_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtfile = open(\"/home/kriselda/Downloads/arrival.txt\", \"r\")\n",
    "ARRIVAL = srtfile.read()\n",
    "srtfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"full_episode.txt\", \"r\") as srtfile:\n",
    "    NANNY = srtfile.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_text_from_srt\n",
    "NICEGUYS=get_text_from_srt('theniceguys.srt')\n",
    "OTHER = get_text_from_srt('theotherguys.srt')\n",
    "B99 = get_text_from_srt('b99.srt')\n",
    "LO = get_text_from_srt('lawandorder.srt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import divide_text_into_sentence_batches\n",
    "input_batches = divide_text_into_sentence_batches(input_text,max_tokens_per_batch=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#flan is not working for summarization\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "for input_text_n in input_batches:\n",
    "    input_ids = tokenizer('Summarize : '+ input_text_n, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(input_ids)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BartTokenizer, BartForConditionalGeneration\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# philschmid/bart-large-cnn-samsum is better that flan-t5 but not acceptable\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load pre-trained BART tokenizer and model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBartTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphilschmid/flan-t5-base-samsum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m BartForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphilschmid/bart-large-cnn-samsum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Input text to be summarized\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tts/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2027\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tts/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2261\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2261\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2266\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tts/lib/python3.12/site-packages/transformers/models/bart/tokenization_bart.py:209\u001b[0m, in \u001b[0;36mBartTokenizer.__init__\u001b[0;34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    207\u001b[0m mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(vocab_handle)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "# philschmid/bart-large-cnn-samsum is better that flan-t5 but not acceptable\n",
    "# Load pre-trained BART tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained('philschmid/flan-t5-base-samsum')\n",
    "model = BartForConditionalGeneration.from_pretrained('philschmid/bart-large-cnn-samsum')\n",
    "\n",
    "# Input text to be summarized\n",
    "for input_text_n in input_batches:\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(input_ids, max_length=200, min_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the generated summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print the generated summary\n",
    "    print(\"Generated Summary:\")\n",
    "    print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "# Prompt for summarization\n",
    "for prompt in input_batches:\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(prompt + '/n : summarize', return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate output\n",
    "    output = model.generate(input_ids, max_length=150, num_return_sequences=1, early_stopping=True)\n",
    "\n",
    "    # Decode and print the summarized text\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Summarized text:\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Pimento returns to the NYPD after spending time in prison in Uzbekistan. Pimento and Jake plan to get married in 14 hours. Jake asks Rafael UPD for a wedding.'}]\n",
      "[{'summary_text': '\"Empire\" is about two best friends getting married in New York City. The show is set to air on Sunday night at 10 p.m. ET.'}]\n",
      "[{'summary_text': \"Jake and Rosa are trying to find his grandma's earrings. The store where the earrings were bought burned down. Jake decides to ignore the sign and marry Rosa anyway.\"}]\n",
      "[{'summary_text': \"Rosa is drunk off champers at a Renaissance Faire. She asks Charles to marry her, but he says he's not sober enough. Rosa calls the local police and asks them to get a warrant. Rosa and her friends rob a store and try to steal the owner's earrings. They find the earrings, then light the store on fire.\"}]\n",
      "[{'summary_text': 'Adrian: \"I\\'m so sick of this garbage, all right? The two of you need to stop looking for signs everywhere... oh, my God, a sign. Whoo! He can fly! Prop planes! You can fly, you can fly!\"'}]\n",
      "[{'summary_text': 'Adrian: \"I spent the whole day denying the fact that there were signs, but I was wrong\" \"I\\'m a part of this journey. Don\\'t you dare try to cut me out\"'}]\n",
      "[{'summary_text': 'In just 14 hours Amy put together the most amazing non-wedding wedding in history. Better than \"Sleepless in Seattle.\" Nope. That is also Nora Ephron.'}]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from utils import divide_text_into_sentence_batches\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "batches = divide_text_into_sentence_batches(B99,max_tokens_per_batch=900)\n",
    "for i in range(len(batches)):\n",
    "\n",
    "    print(summarizer(batches[i], max_length=130, min_length=30, do_sample=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OTHER[:20000].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ook.\\n\"Opening night, 9:00 p.m.\" Signed, Chet.\\n- Fucking Chet. - The protestor guy?\\nGive me that shit.\\nShe was planning something with Chet.\\nOpening night?\\nThe LA auto show.\\n- It\\'s today, right? - Yeah.\\nBig party. Mucky-mucks. Loads of press.\\nIf you wanted to get a story out there, right?\\nAnd fucking Chet\\'s a projectionalist.\\nPlease stop talking.\\nI\\'ve been listening to everything you said.\\nDoes this mean... Does this mean that my niece is dead?\\nYes!\\nI mean, you know, yes.\\n- She was murdered. I\\'m sorry. - Mmm-hmm.\\nBut we\\'re gonna bring down the people who did it.\\nYeah, and for a deeply discounted rate, so...\\nWelcome to Los Angeles and the 1978 Pacific Coast Auto Show.\\nStyled road wheels.\\nThese all-new fuel-efficient systems.\\nAn incredible 210-brake horsepower.\\nTheir battery-powered three-passenger runabout.\\nCadillac have brought their new El Dorado.\\nFeaturing a 7-liter V8.\\nYou guys know where the projection room is?\\nYou seen Chet, the projectionist?\\nYeah, he just left like 10 minutes ago, went for a drink.\\nAnd you are?\\nIn a hurry. Thanks, buddy.\\nHow\\'d you know my name was Buddy?\\nThe Malibu sedan.\\nThis year\\'s edition is looking spectacular.\\nCome and see it, folks.\\nIt\\'s just a bunch of cars.\\nMotor City Pride.\\n- That\\'s not it. - Shit.\\nThat\\'s not the film.\\nFucking Chet.\\nHe\\'s probably still got it stashed somewhere.\\nTally. Oh, my God, you look incredible.\\nHow do you get your hair to... It\\'s magnificent.\\nListen, I don\\'t know what\\'s going on here,\\nbut there\\'s been some foul play.\\nDo you know that that suitcase that you gave us,\\nsomebody switched it out?\\nThere was no money in it.\\nNo shit.\\nWeapons on the floor. Now.\\nI guess you killed the projectionist, huh?\\nNo. My associate\\'s out looking for him now.\\nWe\\'ll find him.\\nTally, let me ask you something.\\nYou ever really killed anybody?\\nIn Detroit, yeah. Three times.\\nReally?\\nThat\\'s where this all started. The Detroit show.\\nThat bitch Misty shooting her mouth off about her new movie.\\nTally, this is not you.\\nYou\\'re not a murderer.\\nShe just said she killed three people.\\nI know, but I\\'m saying deep down.\\nHey, look, one\\'s a mistake.\\n- By the time you get to three... - Don\\'t paint her with that brush.\\nI guess it\\'s easy to live in your world, right,\\nwhere everyone sits in their place.\\nSee what\\'s in front of you.\\nShe\\'s got a gun and she\\'s killed three...\\nCome on, man.\\nYou don\\'t know her upbringing.\\n- You gotta face the situation. - You don\\'t know what she...\\nRoom service.\\n- Shit. No. - What\\'s wrong with him?\\nI don\\'t know. I\\'m gonna ask him.\\n- March? - Yeah?\\nUh, what the fuck are you doing?\\nDid you move it?\\n- Move what? - The fucking gun.\\n- What gun? - The fucking ankle gun.\\nWho told you I had an ankle gun?\\nYou did. In the car before we crashed.\\nYou were like, \"Oh, check out my ankle gun.\"\\nYou know, you showed me your ankle gun.\\nCome on. Are you serious? Are you fucking serious?\\n- Oh, shit. - Yeah.\\n- Did I dream that? - Yeah, you moron, you dreamt it.\\nNo, no, no, no.\\nYeah, you\\'re right, that was...\\nJust shut up! Shut up! Both of you.\\nRoom service.\\n- This takes the fucking cake. - Shh!\\nHolly, you can come in now.\\nVery clever, Holly.\\nThanks. I thought so.\\nWhy did you just throw cold coffee on me?\\nI got it in the hallway. I thought it was hot.\\nI like where your head\\'s at, sweetheart.\\nThat really could have worked out.\\nAll right, you know, everybody, in the corner. Come on.\\nShit.\\n- Well, that really worked out. - Yeah.\\nNow we just gotta find that fucking Chet before John Boy does.\\nYeah. Well, that guy said he was going for a drink.\\nYou take the roof bar. I\\'ll take downstairs.\\n- Well done, kiddo. - Thanks.\\n- You the projectionist? - Mmm-hmm.\\nLook, we got a problem on nine.\\nSomeone knocked over the projector.\\nThe film\\'s all over the floor.\\n- Film\\'s on the floor? Really? - Yeah, it\\'s a mess.\\nYou follow me?\\nOkay, just wait here. I\\'m gonna take a look around.\\nI wanna help.\\nYou can help by staying put, okay?\\nPromise me you\\'ll get the film?\\nYeah, I promise.\\nPinky promise?\\nFuck.\\nHey, pal, what can I do you for?\\nFree drinks. What do you have?\\nLittle guy, stringy hair.\\nI think they went out through those doors.\\nHey, Chet? Chet?\\nHey. Hey, Chet.\\nAmelia\\'s film, where is it?\\nThe film is in the projector. Repeat, it\\'s in the projector.\\nBut we already checked that.\\n- Spliced in. - What?\\nIt\\'s spliced into the middle of it, right in the other film. Go get it.\\nOn my WHY-\\nDon\\'t you know it\\'s rude to eavesdrop?\\nI got a gun pointed directly at your daughter\\'s spine.\\nCome with me. Come on, baby.\\nHow does that song go?\\nWelcome, Los Angeles,\\nto the finest fleet of automobiles Detroit has to offer,\\ngiving the world luxury redefined.\\nIn addition to the most distinctive styling,\\nwe are bringing you interiors that are comfort assured,\\ncombining velour, leather, wood paneling and an improved...\\nHelp him up.\\nWhy\\'d you have to bring the goddamn kid?\\nI fucked up.\\nYeah, you fucked up.\\nAmerica...\\nOh, my God.\\nFuck, fuck.\\nDo you want her to see you like this?\\nYou fucking drunk.\\nOh, don\\'t start that crying shit.\\nI fucked up.\\nYou drunk motherfucker, you.\\n- I love you. - It\\'s embarrassing.\\nI\\'m sorry. Duck.\\n- What? - Duck.\\nMotherfucker!\\nWell, I\\'m Bulging Paulsen\\nand I represent the Detroit auto manufacturers.\\nThat\\'s who the hell I am.\\nYou poison our air. The people won\\'t stand for it.\\nNixon!\\nNo!\\nI might be persuaded to change my mind.\\nPerhaps if we came to a monetary arrangement.\\nI\\'ll take a wire transfer. It\\'s a Union Federal account.\\nNumber 22-12.\\nJust tell them the exact amounts.\\nHe\\'s got a gun!\\nOh, my God.\\nThree, two, one.\\nJesus!\\nHow\\'d you get down here? I told you to go to the roof.\\n- Did you fall? - Yeah.\\nJesus Christ, are you kidding?\\nI think I\\'m invincible.\\nIt\\'s the only thing that makes sense.\\n- I don\\'t think I can die. - Where\\'s the film?\\nIt\\'s up there. We just gotta go get it.\\nYou get out of here, you little shit.\\nYou give me that, you fucked-up little hippy.\\nYou want it? Go get it.\\nNo!\\nGet me that fucking film. Move it.\\n- Cover me. - What... March! March!\\nDrop your weapon!\\nFuck.\\nMarch, go. I got this.\\nHey!\\nRun! Just run!\\nMr. Healy, what are you doing?\\nGo away, Holly.\\nHealy, stop! You don\\'t have to kill him!\\nMr. Healy, if you kill this man, I will never speak to you again.\\nCongratulations, buddy.\\nYou owe your life to a 13-year-old girl.\\nCome on, let\\'s go down and see your dad.\\nAnd that would be the cops.\\nSir? Sir, are you all right?\\nSir, is anyone left in the building?\\nHe\\'s not responsive.\\nAll right, let\\'s find out who else is...\\nAnd sometimes... Sometimes,\\nyou just win.\\nJesus Christ.\\nOh, shit.\\nYou know what? Don\\'t even talk to her.\\nDon\\'t even look at her, man.\\nFuck.\\nOh, boys, boys.\\nYou really think you\\'ve got something done here.\\nDo you have a clue what just happened?\\nIt was protocol. I followed protocol.\\nWhat\\'s wrong with him?\\nI believe he\\'s making a connection between you and Adolf Hitler.\\nRead the fucking newspaper.\\nWhat\\'s good for Detroit is good for America.\\nUnbelievable.\\nThe America I love owes its life to the Big Three.\\nBut it\\'s all right for you to fail your daughter?\\nDetroit had her killed.\\nI think I read about that.\\nThe whole city got together, took a vote. Big turnout.\\nI wanted her safe.\\nThat\\'s why I hired you two.\\nYou\\'re going to jail, Mrs. Kuttner.\\nI might be going to jail, but it won\\'t make a difference.\\nYou can\\'t take Detroit down.\\nAnd if I\\'m not there to take care of it,\\nsomeone else will be.\\nOkay, well, we shall see.\\nHo, ho, ho! Merry Christmas, everybody. Merry Christmas!\\nJesus.\\nWhere is he?\\nScotch.\\nDid you see the TV?\\nYeah, I saw.\\nThey\\'re gonna let them off, the car companies, soot-free.\\nNot enough evidence of collusion, you see.\\nI heard.\\nThe sun went up, the sun went down.\\nNothing changes, just like you said.\\nLook, they got away with it. Big surprise, you know?\\nPeople are stupid. But they\\'re not that stupid.\\nThe point is five years tops,\\nwe\\'re all driving electric cars from Japan, anyway.\\nMark my words.\\nLook at this.\\nYou ever see the bad-breath tie?\\nBreathe on it.\\nWorks every time. Kills Holly.\\nAt least you\\'re drinking again.\\nYeah. I feel great.\\nYou know, nobody got hurt.\\nA few people got hurt.\\nI\\'m saying I think they died quickly, though,\\nso I don\\'t think that they got hurt.\\nLook at this.\\nI\\'m sorry you look Filipino- JACKSON: I do.\\nOr I look Mexican.\\nAnd we already got our first case.\\n- Old lady in Glendale. - Mmm-hmm.\\nThinks her husband\\'s sleeping with Lynda Carter.\\nWonder Woman?\\nOr Lynda Carter.\\nThat\\'s what we have to figure out.\\nRight.\\n- But he\\'s 82, so it\\'s time sensitive. - Hmm.\\nWhat do you say?\\nShit.\\nTo the birds.\\nHallelujah.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NICEGUYS[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22203"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(B99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3586"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(B99.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
